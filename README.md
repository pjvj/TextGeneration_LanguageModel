# TextGeneration_LanguageModel

### Building Character Level LANGUAGE MODEL using Decoder Only Transformer following the Self Attention Mechanism.
We use a Shakespeare dataset to train the Transformer Model using the Self Attention Mechanism described in "Attention is All You Need" paper. Our Language Model learns Shakespeare's work on a character level and generates similar content that closely resembles Shakespeare's work in the training set.

Note: It is a character-level training model and not a word/token level. So it cannot generate real content. It only generates real-like content which may or may not mean anything.





References:
- Attention is All You Need paper: https://arxiv.org/abs/1706.03762
- https://www.youtube.com/watch?v=kCc8FmEb1nY
